{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from func import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_sens_vector_(df, dataset, sens_dict):\n",
    "    if dataset in ['Walmart-Amazon', 'Fodors-Zagat', 'COMPAS']:\n",
    "        df['left_contains_s'] = df[sens_dict[dataset][0]] == sens_dict[dataset][1]\n",
    "        \n",
    "    elif dataset in ['Beer', 'Amazon-Google', 'iTunes-Amazon', 'DBLP-GoogleScholar']:\n",
    "        df[sens_dict[dataset][0]] = df[sens_dict[dataset][0]].astype(str)\n",
    "        df['left_contains_s'] = df[sens_dict[dataset][0]].str.lower().str.contains(sens_dict[dataset][1].lower())\n",
    "    else:\n",
    "        # df[sens_dict[dataset][0]] = df[sens_dict[dataset][0]].astype(str)\n",
    "        # df['left_contains_s'] = df[sens_dict[dataset][0]].apply(lambda x: x.replace('&#216;', '').replace('&#214;', '').replace('&#237;', ',').split(',')[-1].strip())\n",
    "        # df['left_contains_s'] = df['left_contains_s'].apply(lambda x: ', '.join([gender_rev(name) for name in x.split(',')]))\n",
    "        # df['left_contains_s'] = df['left_contains_s'].apply(lambda x: 'True' if 'female' in str(x) else 'False')\n",
    "        # df['left_contains_s'] = df['left_contains_s'].apply(lambda x: any(item in x for item in ['True']))\n",
    "\n",
    "\n",
    "        df[sens_dict[dataset][0]] = df[sens_dict[dataset][0]].astype(str)\n",
    "        df['left_contains_s'] = df[sens_dict[dataset][0]].apply(lambda x: x.replace('&#216;', '').replace('&#214;', '').replace('&#237;', ',').split(',')[-1].strip())\n",
    "\n",
    "\n",
    "        sens = [] \n",
    "        for x  in list(df['left_contains_s']):\n",
    "            try: \n",
    "                tmp = str(gender_rev(x))\n",
    "            except:\n",
    "                tmp = '?'\n",
    "            if 'female' in tmp:\n",
    "                GENDER = True\n",
    "            else:\n",
    "                GENDER = False\n",
    "            sens.append(GENDER)\n",
    "\n",
    "        df['left_contains_s'] = sens\n",
    "        result_vector = np.array(df['left_contains_s'])\n",
    "        sens_attr = np.array(result_vector).reshape(-1)\n",
    "        return sens_attr\n",
    "\n",
    "    result_vector = np.array(df['left_contains_s']).astype(int)\n",
    "    sens_attr = np.array(result_vector).reshape(-1)\n",
    "\n",
    "    return sens_attr\n",
    "\n",
    "\n",
    "\n",
    "for path in os.listdir('data/'):\n",
    "    if 'DS_Store' in path: continue\n",
    "    \n",
    "    tableA = pd.read_csv('data/' + path  +'/tableA.csv')\n",
    "    tableB = pd.read_csv('data/' + path  +'/tableB.csv')\n",
    "    golden = pd.read_csv('data/' + path  +'/matches.csv')\n",
    "    \n",
    "    print(path, tableA.shape[0], tableB.shape[0], round((tableA.shape[0] * tableB.shape[0])/1000,1), golden.shape[0])\n",
    "\n",
    "print()\n",
    "\n",
    "for task in [\n",
    "    'Walmart-Amazon',\n",
    "                'Beer',\n",
    "                'Amazon-Google',\n",
    "                'Fodors-Zagat',\n",
    "                'iTunes-Amazon',\n",
    "                'DBLP-GoogleScholar',\n",
    "                'Febrl',\n",
    "                'DBLP-ACM'\n",
    "                ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    folder_root = \"data/\" + task\n",
    "    left_df = pd.read_csv(folder_root +'/tableA.csv')\n",
    "    right_df = pd.read_csv(folder_root +'/tableB.csv')\n",
    "    match_df = pd.read_csv(folder_root +'/matches.csv')\n",
    "    match_df.rename(columns={list(match_df.columns)[0]: 'ltable_id', list(match_df.columns)[1]: 'rtable_id'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    left_sens = make_sens_vector_(left_df.copy(), task, sens_dict)\n",
    "    right_sens = make_sens_vector_(right_df.copy(), task, sens_dict)\n",
    "\n",
    "    \n",
    "    df_A = pd.DataFrame(left_sens.astype(bool))\n",
    "    df_B = pd.DataFrame(right_sens.astype(bool))\n",
    "\n",
    "    match_sens = make_candid_sens(match_df, left_df.copy(), right_df.copy(),task, sens_dict)\n",
    "\n",
    "\n",
    "    print(task , 'A',np.sum(df_A)[0], 'B',np.sum(df_B)[0], np.sum(match_sens['sensitive']))\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Blocking stats, Bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from func import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    "    QGramsBlocking,\n",
    "    SuffixArraysBlocking\n",
    ")\n",
    "from pyjedai.comparison_cleaning import (\n",
    "    WeightedEdgePruning, WeightedNodePruning, \n",
    "    CardinalityEdgePruning, CardinalityNodePruning,\n",
    "    BLAST, ReciprocalCardinalityNodePruning,\n",
    "    ReciprocalWeightedNodePruning, ComparisonPropagation)\n",
    "\n",
    "\n",
    "# Define method dictionaries\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "classic_method_name = {\n",
    "    'SB': 'StandardBlocking',\n",
    "    'EQG': 'ExtendedQGramsBlocking',\n",
    "    'ESA': 'ExtendedSuffixArraysBlocking',\n",
    "    'QG': 'QGramsBlocking',\n",
    "    'SA': 'SuffixArraysBlocking',\n",
    "    'CTT': 'CTT',\n",
    "    'AE': 'AUTO',\n",
    "\n",
    "}\n",
    "\n",
    "output_file = \"block_stat.txt\"\n",
    "# output_file = \"tmp.txt\"\n",
    "\n",
    "# Remove output file if it exists\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Beer', 'Walmart-Amazon', 'Amazon-Google', 'DBLP-ACM', 'Fodors-Zagat', 'DBLP-GoogleScholar']# 'iTunes-Amazon'\n",
    "methods = ['SB', 'EQG', 'ESA', 'QG','SA','CTT','AE']\n",
    "\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        \n",
    "        # Load datasets\n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "        # Clean datasets for specific tasks\n",
    "        if task == 'Fodors-Zagat':\n",
    "            for df in [left_df, right_df]:\n",
    "                df.applymap(lambda x: x.strip('`').strip() if isinstance(x, str) else x)\n",
    "                df.applymap(lambda x: x.strip(\"'\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "        left_df = left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df = right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        \n",
    "        left_df = left_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        right_df = right_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "\n",
    "        # Process using classic method if applicable\n",
    "        if method in classic_method_dict:\n",
    "            bb = classic_method_dict[method]\n",
    "            attr = [col for col in left_df.columns if col != 'id']\n",
    "            data = Data(\n",
    "                dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "                dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "                ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'})\n",
    "            )\n",
    "            data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            blocks = bb.build_blocks(copy.deepcopy(data), attributes_1=attr, attributes_2=attr, tqdm_disable=True)\n",
    "            \n",
    "            bp = BlockPurging()\n",
    "            bf = BlockFiltering()\n",
    "            mb = BLAST('EJS')\n",
    "            if task == 'iTunes-Amazon':\n",
    "                mb = CardinalityEdgePruning()\n",
    "            # for meta in META:\n",
    "            cleaned_blocks = bf.process(copy.deepcopy(blocks), data, tqdm_disable=True)\n",
    "            filtered_blocks = bp.process(cleaned_blocks, data, tqdm_disable=True)\n",
    "            candidate_pairs_blocks = mb.process(filtered_blocks, data, tqdm_disable=False)\n",
    "\n",
    "            candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "        \n",
    "        else:\n",
    "            if method == 'CTT':\n",
    "                start_time = time.time()\n",
    "                candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = 'CTT')\n",
    "            elif method =='AE':\n",
    "                start_time = time.time()\n",
    "                candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = 'AE')\n",
    "            else:\n",
    "                print('method not found!')\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Compile result rows\n",
    "        candidates= candidates.astype(int)\n",
    "\n",
    "        # Rename columns in candidates DataFrame\n",
    "        candidates.rename(columns={'ltable_id': 'id1', 'rtable_id': 'id2'}, inplace=True)\n",
    "\n",
    "        # Merge left_df and right_df with candidates based on ids using vectorized operations\n",
    "        left_merged = candidates.merge(left_df, left_on='id1', right_on='id', suffixes=('', '_left'))\n",
    "        right_merged = left_merged.merge(right_df, left_on='id2', right_on='id', suffixes=('_left', '_right'))\n",
    "\n",
    "        # Drop redundant columns and reset index\n",
    "        result_df = right_merged.drop(columns=['id1', 'id2', 'id_left', 'id_right'])\n",
    "        result_df = right_merged.copy()\n",
    "\n",
    "        # Merge with match_df to determine labels\n",
    "        merged_df = result_df.merge(match_df, left_on=['id_left', 'id_right'], right_on=['ltable_id', 'rtable_id'], how='left', indicator=True)\n",
    "        result_df['label'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "        # result_df.to_csv(task +'_'+method+ '.csv',index= False)\n",
    "\n",
    "        # Calculate metrics\n",
    "        RR = 1 - result_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        RR_opt = 1 - match_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        PC = np.sum(result_df['label'] == 1) / match_df.shape[0]\n",
    "        PQ = np.sum(result_df['label'] == 1) / result_df.shape[0]\n",
    "        F = 2 * PC * PQ / (PC + PQ)\n",
    "\n",
    "        # Print and save results\n",
    "        print(task, classic_method_name[method])\n",
    "        print(round(100 * RR, 4), round(100 * PC, 4), round(100 * PQ, 4), round(100 * F, 4))\n",
    "        print()\n",
    "\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "        MINOR, MAJOR, data_STAT = calc_bias_block(result_df,match_df,left_df,right_df, task , sens_dict)\n",
    "        [RR_minor,PC_minor,PQ_minor,Fb_minor ] = MINOR\n",
    "        [RR_major,PC_major,PQ_major,Fb_major ] = MAJOR\n",
    "        [P_major, P_minor, M_major, M_minor] = data_STAT\n",
    "\n",
    "        NUM = 2\n",
    "        print('Minor: ',end='')\n",
    "        print(round(RR_minor,NUM),round(PC_minor,NUM),round(PQ_minor,NUM), round(Fb_minor,NUM))\n",
    "        print('major: ',end='')\n",
    "        print(round(RR_major,NUM),round(PC_major,NUM),round(PQ_major,NUM), round(Fb_major,NUM))\n",
    "        print('diff : ',end='')\n",
    "        print(round(RR_major-RR_minor ,NUM),round(PC_major- PC_minor,NUM),round(PQ_major - PQ_minor,NUM),round(Fb_major - Fb_minor,NUM))\n",
    "\n",
    "\n",
    "        print()\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "            file.write(f\"{round(100 * RR, 4)} {round(100 * PC, 4)} {round(100 * PQ, 4)} {round(100 * F, 4)} {round(runtime, 2)}\\n\\n\")\n",
    "            file.write(f\"bias {round( RR_minor, 4)} {round( PC_minor, 4)} {round( PQ_minor, 4)} {round( Fb_minor, 4)}\\n\\n\")\n",
    "            file.write(f\"bias {round( RR_major, 4)} {round( PC_major, 4)} {round( PQ_major, 4)} {round( Fb_major, 4)}\\n\\n\")\n",
    "            file.write(f\"bias {round( RR_major - RR_minor, 4)} {round( PC_major - PC_minor, 4)} {round( PQ_major - PQ_minor, 4)} {round( Fb_major - Fb_minor, 4)}\\n\\n\")\n",
    "            file.write(f\"bias {P_major} {P_minor} {M_major} {M_minor}\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert txt to latex table result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your text file\n",
    "file_path = 'block_stat.txt'\n",
    "\n",
    "# Read the file and process the data\n",
    "data = []\n",
    "res = {}\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into parts based on spaces\n",
    "        parts = line.split()\n",
    "        if parts == []: continue\n",
    "        if len(parts) == 2:\n",
    "            if parts[0] not in res.keys():\n",
    "                res[parts[0]] = {}\n",
    "            res[parts[0]][parts[1]] = {}\n",
    "            method = parts[1]\n",
    "            dataset = parts[0]\n",
    "        else:\n",
    "            RR = float(parts[0])\n",
    "            PC = float(parts[1])\n",
    "            PQ = float(parts[2])\n",
    "            time_ = float(parts[4])\n",
    "            Fb = 2*PC *RR / (PC + RR)\n",
    "            res[dataset][method] = {'RR':round(RR,5), 'PC':round(PC,5), 'PQ':round(PQ,5), 'Fb':round(Fb,5), 'time':round(time_,5)}\n",
    "\n",
    "\n",
    "METHODS = {\n",
    "    'StandardBlocking':'\\\\stdBlock',\n",
    "    'QGramsBlocking':'\\\\qgram',\n",
    "    'ExtendedQGramsBlocking':'\\\\exQgram',\n",
    "    'SuffixArraysBlocking':'\\\\suffix',\n",
    "    'ExtendedSuffixArraysBlocking':'\\\\exSuffix',\n",
    "    'AUTO':'\\\\AutoBlock',\n",
    "    'CTT':'\\\\CTT'}\n",
    "\n",
    "# TASKS = ['Amazon-Google','Walmart-Amazon','DBLP-GoogleScholar', 'DBLP-ACM','Beer','Fodors-Zagat','iTunes-Amazon']\n",
    "# for method in METHODS.keys():\n",
    "#     for task in TASKS:\n",
    "    \n",
    "#         row = res[task][method]\n",
    "#         if task == 'iTunes-Amazon':\n",
    "#             print(f\"{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {row['Fb']:.2f} \\\\\\\\\")\n",
    "#         elif task == 'Amazon-Google':\n",
    "#             print(f\"{METHODS[method]} & \\n{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {row['Fb']:.2f} & \")\n",
    "#         else:\n",
    "#             print(f\"{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {row['Fb']:.2f} & \")\n",
    "\n",
    "\n",
    "#     print()\n",
    "\n",
    "\n",
    "TASKS = ['Amazon-Google', 'Walmart-Amazon', 'DBLP-GoogleScholar', 'DBLP-ACM', 'Beer', 'Fodors-Zagat', 'iTunes-Amazon']\n",
    "\n",
    "# Open a file in write mode\n",
    "with open('exp1_latex.txt', 'w') as file:\n",
    "    for method in METHODS.keys():\n",
    "        for task in TASKS:\n",
    "            row = res[task][method]\n",
    "            if task == 'iTunes-Amazon':\n",
    "                file.write(f\"{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {row['Fb']:.2f} \\\\\\\\\\n\")\n",
    "            elif task == 'Amazon-Google':\n",
    "                file.write(f\"{METHODS[method]} & \\n{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {row['Fb']:.2f} & \\n\")\n",
    "            else:\n",
    "                file.write(f\"{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {row['Fb']:.2f} & \\n\")\n",
    "\n",
    "        # Add a newline after each method\n",
    "        file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation of RR, PC, PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize the overall correlation matrix\n",
    "correlation_matrix_all = np.zeros((3, 3))\n",
    "\n",
    "# Iterate over tasks\n",
    "for i in range(7):\n",
    "    RR = []\n",
    "    PC = []\n",
    "    PQ = []\n",
    "    \n",
    "    # Calculate metrics for each method\n",
    "    for method in METHODS.keys():\n",
    "        task = TASKS[i]\n",
    "        row = res[task][method]\n",
    "        RR.append(row['RR'])\n",
    "        PC.append(row['PC'])\n",
    "        PQ.append(row['PQ'])\n",
    "\n",
    "    # Convert metrics to numpy arrays\n",
    "    rr = np.array(RR)\n",
    "    pc = np.array(PC)\n",
    "    pq = np.array(PQ)\n",
    "\n",
    "    # Create a DataFrame and compute the correlation matrix\n",
    "    data = pd.DataFrame({'RR': rr, 'PC': pc, 'PQ': pq})\n",
    "    correlation_matrix_all += np.abs(data.corr())\n",
    "\n",
    "# Plot the overall correlation heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "colorblind_palette = sns.color_palette(\"colorblind\", as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    (correlation_matrix_all / 7), \n",
    "    annot=True, \n",
    "    cmap=colorblind_palette, \n",
    "    center=0, \n",
    "    annot_kws={\"size\": 40, \"weight\": \"bold\"},\n",
    "    cbar=False\n",
    ")\n",
    "\n",
    "plt.xticks(fontsize=60)\n",
    "plt.yticks(fontsize=60)\n",
    "plt.tick_params(axis='both', which='both', length=10, width=3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('correlation_heatmap_all.pdf')\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from func import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    "    QGramsBlocking,\n",
    "    SuffixArraysBlocking\n",
    ")\n",
    "from pyjedai.comparison_cleaning import BLAST, CardinalityEdgePruning\n",
    "\n",
    "\n",
    "# Define method dictionaries\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "classic_method_name = {\n",
    "    'SB': 'StandardBlocking',\n",
    "    'EQG': 'ExtendedQGramsBlocking',\n",
    "    'ESA': 'ExtendedSuffixArraysBlocking',\n",
    "    'QG': 'QGramsBlocking',\n",
    "    'SA': 'SuffixArraysBlocking',\n",
    "    'CTT': 'CTT',\n",
    "    'AE': 'AUTO',\n",
    "\n",
    "}\n",
    "\n",
    "output_file = \"tmp.txt\"\n",
    "# output_file = \"tmp.txt\"\n",
    "\n",
    "# Remove output file if it exists\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['iTunes-Amazon']\n",
    "methods = ['SB', 'EQG', 'ESA', 'QG','SA']\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        \n",
    "        # Load datasets\n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "        # Clean datasets for specific tasks\n",
    "        if task == 'Fodors-Zagat':\n",
    "            for df in [left_df, right_df]:\n",
    "                df.applymap(lambda x: x.strip('`').strip() if isinstance(x, str) else x)\n",
    "                df.applymap(lambda x: x.strip(\"'\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "        left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "\n",
    "        # Process using classic method if applicable\n",
    "        if method in classic_method_dict:\n",
    "            bb = classic_method_dict[method]\n",
    "            attr = [col for col in left_df.columns if col != 'id']\n",
    "            data = Data(\n",
    "                dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "                dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "                ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'})\n",
    "            )\n",
    "            data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            blocks = bb.build_blocks(copy.deepcopy(data), attributes_1=attr, attributes_2=attr, tqdm_disable=True)\n",
    "            \n",
    "            bp = BlockPurging()\n",
    "            bf = BlockFiltering()\n",
    "            mb = BLAST('EJS')\n",
    "            if task == 'iTunes-Amazon':\n",
    "                mb = CardinalityEdgePruning()\n",
    "\n",
    "            cleaned_blocks = bf.process(blocks, data, tqdm_disable=True)\n",
    "            filtered_blocks = bp.process(cleaned_blocks, data, tqdm_disable=True)\n",
    "            candidate_pairs_blocks = mb.process(filtered_blocks, data, tqdm_disable=False)\n",
    "\n",
    "            candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "            end_time = time.time()\n",
    "\n",
    "        candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "        # Compile result rows\n",
    "        candidates= candidates.astype(int)\n",
    "\n",
    "        # Rename columns in candidates DataFrame\n",
    "        candidates.rename(columns={'ltable_id': 'id1', 'rtable_id': 'id2'}, inplace=True)\n",
    "\n",
    "        # Merge left_df and right_df with candidates based on ids using vectorized operations\n",
    "        left_merged = candidates.merge(left_df, left_on='id1', right_on='id', suffixes=('', '_left'))\n",
    "        right_merged = left_merged.merge(right_df, left_on='id2', right_on='id', suffixes=('_left', '_right'))\n",
    "\n",
    "        # Drop redundant columns and reset index\n",
    "        result_df = right_merged.drop(columns=['id1', 'id2', 'id_left', 'id_right'])\n",
    "        result_df = right_merged.copy()\n",
    "\n",
    "        # Merge with match_df to determine labels\n",
    "        merged_df = result_df.merge(match_df, left_on=['id_left', 'id_right'], right_on=['ltable_id', 'rtable_id'], how='left', indicator=True)\n",
    "        result_df['label'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "        result_df.to_csv(task +'_'+method+ '.csv',index= False)\n",
    "\n",
    "        # Calculate metrics\n",
    "        RR = 1 - result_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        RR_opt = 1 - match_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        PC = np.sum(result_df['label'] == 1) / match_df.shape[0]\n",
    "        PQ = np.sum(result_df['label'] == 1) / result_df.shape[0]\n",
    "        F = 2 * PC * PQ / (PC + PQ)\n",
    "\n",
    "        # Print and save results\n",
    "        print(task, classic_method_name[method])\n",
    "        print(round(100 * RR, 4), round(100 * PC, 4), round(100 * PQ, 4), round(100 * F, 4))\n",
    "        print()\n",
    "\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "            file.write(f\"{round(100 * RR, 4)} {round(100 * PC, 4)} {round(100 * PQ, 4)} {round(100 * F, 4)} {round(runtime, 2)}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohammad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6b4dd0100645b9bfc7b176e050f4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Comparison Propagation:   0%|          | 0/6907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from func import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    "    QGramsBlocking,\n",
    "    SuffixArraysBlocking\n",
    ")\n",
    "from pyjedai.comparison_cleaning import BLAST, CardinalityEdgePruning\n",
    "from pyjedai.comparison_cleaning import (\n",
    "    WeightedEdgePruning, WeightedNodePruning, \n",
    "    CardinalityEdgePruning, CardinalityNodePruning,\n",
    "    BLAST, ReciprocalCardinalityNodePruning,\n",
    "    ReciprocalWeightedNodePruning, ComparisonPropagation)\n",
    "\n",
    "\n",
    "# Define method dictionaries\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "classic_method_name = {\n",
    "    'SB': 'StandardBlocking',\n",
    "    'EQG': 'ExtendedQGramsBlocking',\n",
    "    'ESA': 'ExtendedSuffixArraysBlocking',\n",
    "    'QG': 'QGramsBlocking',\n",
    "    'SA': 'SuffixArraysBlocking',\n",
    "    'CTT': 'CTT',\n",
    "    'AE': 'AUTO',\n",
    "\n",
    "}\n",
    "\n",
    "output_file = \"block_stat.txt\"\n",
    "# output_file = \"tmp.txt\"\n",
    "\n",
    "# Remove output file if it exists\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Beer', 'Walmart-Amazon', 'Amazon-Google', 'DBLP-ACM', 'Fodors-Zagat', 'iTunes-Amazon', 'DBLP-GoogleScholar', 'Febrl']\n",
    "tasks = ['iTunes-Amazon']\n",
    "methods = ['SB', 'EQG', 'ESA', 'QG','SA','CTT','AE']\n",
    "methods = ['SB']\n",
    "\n",
    "META = [ \n",
    "    # CardinalityEdgePruning(),\n",
    "\n",
    "    ComparisonPropagation()\n",
    "]\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        \n",
    "        # Load datasets\n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "        # Clean datasets for specific tasks\n",
    "        if task == 'Fodors-Zagat':\n",
    "            for df in [left_df, right_df]:\n",
    "                df.applymap(lambda x: x.strip('`').strip() if isinstance(x, str) else x)\n",
    "                df.applymap(lambda x: x.strip(\"'\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "        left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "\n",
    "        # Process using classic method if applicable\n",
    "        if method in classic_method_dict:\n",
    "            bb = classic_method_dict[method]\n",
    "            attr = [col for col in left_df.columns if col != 'id']\n",
    "            data = Data(\n",
    "                dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "                dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "                ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'})\n",
    "            )\n",
    "            data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            blocks = bb.build_blocks(copy.deepcopy(data), attributes_1=attr, attributes_2=attr, tqdm_disable=True)\n",
    "            \n",
    "            bp = BlockPurging()\n",
    "            bf = BlockFiltering()\n",
    "            mb = BLAST('EJS')\n",
    "            if task == 'iTunes-Amazon':\n",
    "                mb = CardinalityEdgePruning()\n",
    "            for meta in META:\n",
    "                mb = copy.deepcopy(meta)\n",
    "                cleaned_blocks = bf.process(copy.deepcopy(blocks), data, tqdm_disable=True)\n",
    "                filtered_blocks = bp.process(cleaned_blocks, data, tqdm_disable=True)\n",
    "                candidate_pairs_blocks = mb.process(filtered_blocks, data, tqdm_disable=False)\n",
    "\n",
    "                candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "        \n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                # Compile result rows\n",
    "                candidates= candidates.astype(int)\n",
    "\n",
    "                # Rename columns in candidates DataFrame\n",
    "                candidates.rename(columns={'ltable_id': 'id1', 'rtable_id': 'id2'}, inplace=True)\n",
    "\n",
    "                # Merge left_df and right_df with candidates based on ids using vectorized operations\n",
    "                left_merged = candidates.merge(left_df, left_on='id1', right_on='id', suffixes=('', '_left'))\n",
    "                right_merged = left_merged.merge(right_df, left_on='id2', right_on='id', suffixes=('_left', '_right'))\n",
    "\n",
    "                # Drop redundant columns and reset index\n",
    "                result_df = right_merged.drop(columns=['id1', 'id2', 'id_left', 'id_right'])\n",
    "                result_df = right_merged.copy()\n",
    "\n",
    "                # Merge with match_df to determine labels\n",
    "                merged_df = result_df.merge(match_df, left_on=['id_left', 'id_right'], right_on=['ltable_id', 'rtable_id'], how='left', indicator=True)\n",
    "                result_df['label'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "                result_df.to_csv(task +'_'+method+'_'+str(meta)+ '.csv',index= False)\n",
    "\n",
    "                # Calculate metrics\n",
    "                RR = 1 - result_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "                RR_opt = 1 - match_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "                PC = np.sum(result_df['label'] == 1) / match_df.shape[0]\n",
    "                PQ = np.sum(result_df['label'] == 1) / result_df.shape[0]\n",
    "                F = 2 * PC * PQ / (PC + PQ)\n",
    "\n",
    "                # Print and save results\n",
    "                print(task, classic_method_name[method])\n",
    "                print(round(100 * RR, 4), round(100 * PC, 4), round(100 * PQ, 4), round(100 * F, 4))\n",
    "                print()\n",
    "\n",
    "                runtime = end_time - start_time\n",
    "\n",
    "                with open(output_file, \"a\") as file:\n",
    "                    file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "                    file.write(f\"{round(100 * RR, 4)} {round(100 * PC, 4)} {round(100 * PQ, 4)} {round(100 * F, 4)} {round(runtime, 2)}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
