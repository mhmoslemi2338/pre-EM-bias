{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from func import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_sens_vector_(df, dataset, sens_dict):\n",
    "    if dataset in ['Walmart-Amazon', 'Fodors-Zagat', 'COMPAS']:\n",
    "        df['left_contains_s'] = df[sens_dict[dataset][0]] == sens_dict[dataset][1]\n",
    "        \n",
    "    elif dataset in ['Beer', 'Amazon-Google', 'iTunes-Amazon', 'DBLP-GoogleScholar']:\n",
    "        df[sens_dict[dataset][0]] = df[sens_dict[dataset][0]].astype(str)\n",
    "        df['left_contains_s'] = df[sens_dict[dataset][0]].str.lower().str.contains(sens_dict[dataset][1].lower())\n",
    "    else:\n",
    "        df[sens_dict[dataset][0]] = df[sens_dict[dataset][0]].astype(str)\n",
    "        df['left_contains_s'] = df[sens_dict[dataset][0]].apply(lambda x: x.replace('&#216;', '').replace('&#214;', '').replace('&#237;', ',').split(',')[-1].strip())\n",
    "\n",
    "\n",
    "        sens = [] \n",
    "        for x  in list(df['left_contains_s']):\n",
    "            try: \n",
    "                tmp = str(gender_rev(x))\n",
    "            except:\n",
    "                tmp = '?'\n",
    "            if 'female' in tmp:\n",
    "                GENDER = True\n",
    "            else:\n",
    "                GENDER = False\n",
    "            sens.append(GENDER)\n",
    "\n",
    "        df['left_contains_s'] = sens\n",
    "        result_vector = np.array(df['left_contains_s'])\n",
    "        sens_attr = np.array(result_vector).reshape(-1)\n",
    "        return sens_attr\n",
    "\n",
    "    result_vector = np.array(df['left_contains_s']).astype(int)\n",
    "    sens_attr = np.array(result_vector).reshape(-1)\n",
    "\n",
    "    return sens_attr\n",
    "\n",
    "\n",
    "\n",
    "for path in os.listdir('data/'):\n",
    "    if 'DS_Store' in path: continue\n",
    "    \n",
    "    tableA = pd.read_csv('data/' + path  +'/tableA.csv')\n",
    "    tableB = pd.read_csv('data/' + path  +'/tableB.csv')\n",
    "    golden = pd.read_csv('data/' + path  +'/matches.csv')\n",
    "    \n",
    "    print(path, tableA.shape[0], tableB.shape[0], round((tableA.shape[0] * tableB.shape[0])/1000,1), golden.shape[0])\n",
    "\n",
    "print()\n",
    "\n",
    "for task in [\n",
    "    'Walmart-Amazon',\n",
    "                'Beer',\n",
    "                'Amazon-Google',\n",
    "                'Fodors-Zagat',\n",
    "                'iTunes-Amazon',\n",
    "                'DBLP-GoogleScholar',\n",
    "                'Febrl',\n",
    "                'DBLP-ACM'\n",
    "                ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    folder_root = \"data/\" + task\n",
    "    left_df = pd.read_csv(folder_root +'/tableA.csv')\n",
    "    right_df = pd.read_csv(folder_root +'/tableB.csv')\n",
    "    match_df = pd.read_csv(folder_root +'/matches.csv')\n",
    "    match_df.rename(columns={list(match_df.columns)[0]: 'ltable_id', list(match_df.columns)[1]: 'rtable_id'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    left_sens = make_sens_vector_(left_df.copy(), task, sens_dict)\n",
    "    right_sens = make_sens_vector_(right_df.copy(), task, sens_dict)\n",
    "\n",
    "    \n",
    "    df_A = pd.DataFrame(left_sens.astype(bool))\n",
    "    df_B = pd.DataFrame(right_sens.astype(bool))\n",
    "\n",
    "    match_sens = make_candid_sens(match_df, left_df.copy(), right_df.copy(),task, sens_dict)\n",
    "\n",
    "\n",
    "    print(task , 'A',np.sum(df_A)[0], 'B',np.sum(df_B)[0], np.sum(match_sens['sensitive']))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for path in os.listdir('data/'):\n",
    "    if 'DS_Store' in path: continue\n",
    "    \n",
    "    tableA = pd.read_csv('data/' + path  +'/tableA.csv')\n",
    "\n",
    "    print(path,tableA.shape[1] -1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Blocking stats, Bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from func import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.comparison_cleaning import CardinalityEdgePruning,BLAST\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    "    QGramsBlocking,\n",
    "    SuffixArraysBlocking\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define method dictionaries\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "classic_method_name = {\n",
    "    'SB': 'StandardBlocking',\n",
    "    'EQG': 'ExtendedQGramsBlocking',\n",
    "    'ESA': 'ExtendedSuffixArraysBlocking',\n",
    "    'QG': 'QGramsBlocking',\n",
    "    'SA': 'SuffixArraysBlocking',\n",
    "    'CTT': 'CTT',\n",
    "    'AE': 'AUTO',\n",
    "\n",
    "}\n",
    "\n",
    "output_file = \"block_stat3_.txt\"\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "# 'Fodors-Zagat', 'Walmart-Amazon', 'Amazon-Google','iTunes-Amazon']\n",
    "\n",
    "tasks = ['DBLP-GoogleScholar' ,'DBLP-ACM','Beer','Fodors-Zagat', 'Walmart-Amazon', 'Amazon-Google','iTunes-Amazon']\n",
    "\n",
    "\n",
    "\n",
    "tasks = ['iTunes-Amazon','Fodors-Zagat', 'Walmart-Amazon', 'Amazon-Google']\n",
    "methods = ['SB', 'EQG', 'ESA', 'QG','SA','CTT','AE']\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        if task =='iTunes-Amazon' and method in ['SB', 'EQG', 'ESA', 'QG','SA']: continue\n",
    "        \n",
    "        # Load datasets\n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "\n",
    "        left_df = left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df = right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        \n",
    "        left_df = left_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        right_df = right_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "\n",
    "        # Process using classic method if applicable\n",
    "        if method in classic_method_dict:\n",
    "            bb = classic_method_dict[method]\n",
    "            attr = [col for col in left_df.columns if col != 'id']\n",
    "            data = Data(\n",
    "                dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "                dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "                ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'}))\n",
    "            data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            blocks = bb.build_blocks(copy.deepcopy(data), attributes_1=attr, attributes_2=attr, tqdm_disable=True)\n",
    "            \n",
    "            bp = BlockPurging()\n",
    "            bf = BlockFiltering()\n",
    "            mb = BLAST('EJS')\n",
    "            if task == 'iTunes-Amazon': mb = CardinalityEdgePruning()\n",
    "            # for meta in META:\n",
    "            cleaned_blocks = bf.process(copy.deepcopy(blocks), data, tqdm_disable=True)\n",
    "            filtered_blocks = bp.process(cleaned_blocks, data, tqdm_disable=True)\n",
    "            candidate_pairs_blocks = mb.process(filtered_blocks, data, tqdm_disable=False)\n",
    "            candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "        \n",
    "        else:\n",
    "            if method == 'CTT':\n",
    "                start_time = time.time()\n",
    "                candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = 'CTT')\n",
    "            elif method =='AE':\n",
    "                start_time = time.time()\n",
    "                candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = 'AE')\n",
    "            else:\n",
    "                print('method not found!')\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "        # Compile result rows\n",
    "        candidates= candidates.astype(int)\n",
    "        candidates.rename(columns={'ltable_id': 'id1', 'rtable_id': 'id2'}, inplace=True)\n",
    "\n",
    "        # Merge left_df and right_df with candidates based on ids using vectorized operations\n",
    "        left_merged = candidates.merge(left_df, left_on='id1', right_on='id', suffixes=('', '_left'))\n",
    "        right_merged = left_merged.merge(right_df, left_on='id2', right_on='id', suffixes=('_left', '_right'))\n",
    "\n",
    "        # Drop redundant columns and reset index\n",
    "        result_df = right_merged.drop(columns=['id1', 'id2', 'id_left', 'id_right'])\n",
    "        result_df = right_merged.copy()\n",
    "\n",
    "        # Merge with match_df to determine labels\n",
    "        merged_df = result_df.merge(match_df, left_on=['id_left', 'id_right'], right_on=['ltable_id', 'rtable_id'], how='left', indicator=True)\n",
    "        result_df['label'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        RR = 1 - result_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        PC = np.sum(result_df['label'] == 1) / match_df.shape[0]\n",
    "        PQ = np.sum(result_df['label'] == 1) / result_df.shape[0]\n",
    "        F = 2 * PC * RR / (PC + RR)\n",
    "\n",
    "        # Print and save results\n",
    "        print(task, classic_method_name[method])\n",
    "        print(round(100 * RR, 4), round(100 * PC, 4), round(100 * PQ, 4), round(100 * F, 4))\n",
    "        print()\n",
    "\n",
    "\n",
    "        res_sens = []\n",
    "        if task == 'DBLP-ACM':\n",
    "            LEFT = pd.DataFrame({'sens':pd.read_csv('data/'+task+'/'+'left_sens.csv')['0'],\n",
    "            'id':pd.read_csv('data/'+task+'/'+'tableA.csv')['id']})\n",
    "\n",
    "\n",
    "            RIGHT = pd.DataFrame({'sens':pd.read_csv('data/'+task+'/'+'right_sens.csv')['0'],\n",
    "            'id':pd.read_csv('data/'+task+'/'+'tableB.csv')['id']})\n",
    "\n",
    "            a = list(result_df['id1'])\n",
    "            b = list(result_df['id2'])\n",
    "\n",
    "\n",
    "            res_sens = []\n",
    "            for i in range(len(a)):\n",
    "                sens_c = np.logical_or(list(RIGHT[RIGHT['id'] == b[i]]['sens'])[0],list(LEFT[LEFT['id'] == a[i]]['sens'])[0])\n",
    "                res_sens.append(sens_c)\n",
    "\n",
    "        print(1)\n",
    "\n",
    "\n",
    "        MINOR, MAJOR, data_STAT = calc_bias_block(result_df,match_df,left_df,right_df, task , sens_dict,res_sens )\n",
    "        [RR_minor,PC_minor,PQ_minor,Fb_minor ] = MINOR\n",
    "        [RR_major,PC_major,PQ_major,Fb_major ] = MAJOR\n",
    "        [P_major, P_minor, M_major, M_minor] = data_STAT\n",
    "\n",
    "        NUM = 2\n",
    "        print('Minor: ',end='')\n",
    "        print(round(RR_minor,NUM),round(PC_minor,NUM),round(PQ_minor,NUM), round(Fb_minor,NUM))\n",
    "        print('major: ',end='')\n",
    "        print(round(RR_major,NUM),round(PC_major,NUM),round(PQ_major,NUM), round(Fb_major,NUM))\n",
    "        print('diff : ',end='')\n",
    "        print(round(RR_major-RR_minor ,NUM),round(PC_major- PC_minor,NUM),round(PQ_major - PQ_minor,NUM),round(Fb_major - Fb_minor,NUM))\n",
    "\n",
    "\n",
    "        print()\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "            file.write(f\"{round(100 * RR,8)} {round(100 * PC,8)} {round(100 * PQ,8)} {round(100 * F,8)} {round(runtime, 2)}\\n\")\n",
    "            file.write(f\"bias {round( RR_minor,8)} {round( PC_minor,8)} {round( PQ_minor,8)} {round( Fb_minor,8)}\\n\")\n",
    "            file.write(f\"bias {round( RR_major,8)} {round( PC_major,8)} {round( PQ_major,8)} {round( Fb_major,8)}\\n\")\n",
    "            file.write(f\"bias {round( RR_major - RR_minor,8)} {round( PC_major - PC_minor,8)} {round( PQ_major - PQ_minor,8)} {round( Fb_major - Fb_minor,8)}\\n\")\n",
    "            file.write(f\"bias {P_major} {P_minor} {M_major} {M_minor}\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.comparison_cleaning import CardinalityEdgePruning,BLAST\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    "    QGramsBlocking,\n",
    "    SuffixArraysBlocking\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define method dictionaries\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "classic_method_name = {\n",
    "    'SB': 'StandardBlocking',\n",
    "    'EQG': 'ExtendedQGramsBlocking',\n",
    "    'ESA': 'ExtendedSuffixArraysBlocking',\n",
    "    'QG': 'QGramsBlocking',\n",
    "    'SA': 'SuffixArraysBlocking',\n",
    "\n",
    "}\n",
    "\n",
    "output_file = \"block_stat_time.txt\"\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Beer','Fodors-Zagat', 'Walmart-Amazon', 'Amazon-Google', 'DBLP-ACM', 'DBLP-GoogleScholar', 'iTunes-Amazon']\n",
    "methods = ['SB', 'EQG', 'ESA', 'QG','SA']\n",
    "\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        \n",
    "        time_all = []\n",
    "        # Load datasets\n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "\n",
    "        left_df = left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df = right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        \n",
    "        left_df = left_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        right_df = right_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "        N= 8\n",
    "        for _ in range(N):\n",
    "        # Process using classic method if applicable\n",
    "            start_time = time.time()\n",
    "\n",
    "            bb = classic_method_dict[method]\n",
    "            attr = [col for col in left_df.columns if col != 'id']\n",
    "            data = Data(\n",
    "                dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "                dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "             ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'}))\n",
    "            data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "            \n",
    "            \n",
    "            blocks = bb.build_blocks(copy.deepcopy(data), attributes_1=attr, attributes_2=attr, tqdm_disable=True)\n",
    "            \n",
    "            bp = BlockPurging()\n",
    "            bf = BlockFiltering()\n",
    "            mb = BLAST('EJS')\n",
    "            if task == 'iTunes-Amazon': mb = CardinalityEdgePruning()\n",
    "            # for meta in META:\n",
    "            cleaned_blocks = bf.process(copy.deepcopy(blocks), data, tqdm_disable=True)\n",
    "            filtered_blocks = bp.process(cleaned_blocks, data, tqdm_disable=True)\n",
    "            candidate_pairs_blocks = mb.process(filtered_blocks, data, tqdm_disable=False)\n",
    "            candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "        \n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            runtime = end_time - start_time\n",
    "        \n",
    "            time_all.append(runtime)\n",
    "        \n",
    "        print()\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "            file.write(f\"{round(np.average(time_all), 4)} {round(np.std(time_all), 4)}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time for same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.comparison_cleaning import CardinalityEdgePruning,BLAST\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    "    QGramsBlocking,\n",
    "    SuffixArraysBlocking\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define method dictionaries\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "classic_method_name = {\n",
    "    'SB': 'StandardBlocking',\n",
    "    'EQG': 'ExtendedQGramsBlocking',\n",
    "    'ESA': 'ExtendedSuffixArraysBlocking',\n",
    "    'QG': 'QGramsBlocking',\n",
    "    'SA': 'SuffixArraysBlocking',\n",
    "\n",
    "}\n",
    "\n",
    "output_file = \"block_stat_time_same5_.txt\"\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Walmart-Amazon']\n",
    "methods = ['SB', 'EQG', 'ESA', 'QG','SA']\n",
    "\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        \n",
    "        time_all = []\n",
    "        # Load datasets\n",
    "        left_df_orig = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df_orig = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df_orig = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        left_df_orig = left_df_orig.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df_orig = right_df_orig.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        \n",
    "        left_df_orig = left_df_orig.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        right_df_orig = right_df_orig.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        SIZE = left_df_orig.shape[0] * right_df_orig.shape[0]\n",
    "\n",
    "        for f in [0.01, 0.2,0.4,0.6,0.8,1]:\n",
    "        # for f in [0.01,0.5,1]:\n",
    "            F = np.sqrt(f)\n",
    "            left_df = copy.deepcopy(left_df_orig).sample(frac =F,replace=False)\n",
    "            right_df = copy.deepcopy(right_df_orig).sample(frac =F,replace=False)\n",
    "            match_df = copy.deepcopy(match_df_orig)\n",
    "            match_df = match_df[match_df['ltable_id'].isin(list(left_df['id']))]\n",
    "            match_df = match_df[match_df['rtable_id'].isin(list(right_df['id']))]\n",
    "\n",
    "        # Process using classic method if applicable\n",
    "            runtime = 0\n",
    "            for _ in range(1):\n",
    "                start_time = time.time()\n",
    "\n",
    "                bb = classic_method_dict[method]\n",
    "                attr = [col for col in left_df.columns if col != 'id']\n",
    "                data = Data(\n",
    "                    dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "                    dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "                ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'}))\n",
    "                data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "                \n",
    "                \n",
    "                blocks = bb.build_blocks(copy.deepcopy(data), attributes_1=attr, attributes_2=attr, tqdm_disable=True)\n",
    "                \n",
    "                bp = BlockPurging()\n",
    "                bf = BlockFiltering()\n",
    "                mb = BLAST('EJS')\n",
    "                # for meta in META:\n",
    "                cleaned_blocks = bf.process(copy.deepcopy(blocks), data, tqdm_disable=True)\n",
    "                filtered_blocks = bp.process(cleaned_blocks, data, tqdm_disable=True)\n",
    "                candidate_pairs_blocks = mb.process(filtered_blocks, data, tqdm_disable=False)\n",
    "                candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "            \n",
    "\n",
    "\n",
    "                end_time = time.time()\n",
    "                runtime+= end_time - start_time\n",
    "                \n",
    "            runtime = runtime /1   \n",
    "            size_ = left_df.shape[0] * right_df.shape[0]\n",
    "            print()\n",
    "            with open(output_file, \"a\") as file:\n",
    "                if f == 0.01:\n",
    "                    file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "                file.write(f\"{round(F**2,2)} {size_},{SIZE} {round(np.average(runtime), 4)}\\n\")\n",
    "                if f == 1:\n",
    "                    file.write(f\"\\n\")\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time for deep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from func import *\n",
    "\n",
    "\n",
    "\n",
    "output_file = \"block_deep_stat_time.txt\"\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Beer','Fodors-Zagat', 'Walmart-Amazon', 'Amazon-Google', 'DBLP-ACM', 'DBLP-GoogleScholar', 'iTunes-Amazon']\n",
    "\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for method in ['AE','CTT']:\n",
    "    for task in tasks:\n",
    "    \n",
    "        \n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "\n",
    "        left_df = left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df = right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        \n",
    "        left_df = left_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        right_df = right_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "        print(task , method)\n",
    "        # start_time = time.time()\n",
    "        candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = method)\n",
    "        # end_time = time.time()\n",
    "        # runtime = end_time - start_time\n",
    "        \n",
    "        \n",
    "        print()\n",
    "        # with open(output_file, \"a\") as file:\n",
    "        #     file.write(f\"{task} {method}\\n\")\n",
    "        #     file.write(f\"{round(np.average(time_all), 4)} {round(np.std(time_all), 4)}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time for same dataset deep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart-Amazon AE 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.740363121032715\n",
      "2.242300033569336\n",
      "2.364861249923706\n",
      "2.329403877258301\n",
      "2.12947678565979\n",
      "Walmart-Amazon AE 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.828837871551514\n",
      "8.53029465675354\n",
      "7.884539842605591\n",
      "7.125627040863037\n",
      "7.668518781661987\n",
      "Walmart-Amazon AE 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.403317928314209\n",
      "10.389057159423828\n",
      "10.333085060119629\n",
      "10.438457012176514\n",
      "10.262450218200684\n",
      "Walmart-Amazon AE 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.923197984695435\n",
      "13.501504182815552\n",
      "12.9218270778656\n",
      "13.242563962936401\n",
      "12.90197491645813\n",
      "Walmart-Amazon AE 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.52104115486145\n",
      "15.437193870544434\n",
      "16.34410309791565\n",
      "15.620084047317505\n",
      "15.052511930465698\n",
      "Walmart-Amazon AE 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.86265993118286\n",
      "18.664067029953003\n",
      "19.72769522666931\n",
      "19.346877813339233\n",
      "18.991119146347046\n",
      "\n",
      "\n",
      "Walmart-Amazon CTT 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.820467948913574\n",
      "2.396394968032837\n",
      "2.1670467853546143\n",
      "2.6886110305786133\n",
      "2.776010036468506\n",
      "Walmart-Amazon CTT 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.641733169555664\n",
      "9.252118110656738\n",
      "9.29131007194519\n",
      "9.17280387878418\n",
      "8.694905996322632\n",
      "Walmart-Amazon CTT 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.919050931930542\n",
      "13.901690244674683\n",
      "14.308176040649414\n",
      "14.26353907585144\n",
      "13.692553997039795\n",
      "Walmart-Amazon CTT 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.114394187927246\n",
      "19.582361936569214\n",
      "17.507019996643066\n",
      "18.043914794921875\n",
      "17.371819972991943\n",
      "Walmart-Amazon CTT 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.47039794921875\n",
      "21.09325909614563\n",
      "20.620249032974243\n",
      "20.82024312019348\n",
      "20.53084373474121\n",
      "Walmart-Amazon CTT 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.74311709403992\n",
      "24.53022003173828\n",
      "23.9623601436615\n",
      "24.175256967544556\n",
      "24.379826068878174\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from func import *\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Walmart-Amazon']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for method in ['AE','CTT']:\n",
    "    for task in tasks:\n",
    "        \n",
    "        left_df_orig = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df_orig = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        \n",
    "        left_df_orig = left_df_orig.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df_orig = right_df_orig.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        \n",
    "        left_df_orig = left_df_orig.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        right_df_orig = right_df_orig.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        SIZE = left_df_orig.shape[0] * right_df_orig.shape[0]\n",
    "\n",
    "        # break\n",
    "        for f in [0.01, 0.2,0.4,0.6,0.8,1]:\n",
    "            F = np.sqrt(f)\n",
    "            left_df = copy.deepcopy(left_df_orig).sample(frac =F,replace=False)\n",
    "            right_df = copy.deepcopy(right_df_orig).sample(frac =F,replace=False)\n",
    "            left_df = left_df.fillna('NA')\n",
    "            right_df = right_df.fillna('NA')\n",
    "\n",
    "\n",
    "\n",
    "            print(task , method, f)\n",
    "            candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = method)\n",
    "        \n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
