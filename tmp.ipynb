{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PBS figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the range of F_PQ_PC values from 0 to 1\n",
    "F_PQ_PC_range = np.linspace(0, 1, 500)\n",
    "\n",
    "def PBS(F_PQ_PC, RR, RR_opt):\n",
    "    \"\"\"\n",
    "    Calculate the PBS value.\n",
    "\n",
    "    Args:\n",
    "        F_PQ_PC (float or array): F_PQ_PC value(s).\n",
    "        RR (float or array): RR value(s).\n",
    "        RR_opt (float): Optimal RR value.\n",
    "\n",
    "    Returns:\n",
    "        array: Calculated PBS values.\n",
    "    \"\"\"\n",
    "    max_denominator = max(1 - RR_opt, RR_opt)\n",
    "    term = 1 - np.sqrt(np.abs(RR - RR_opt) / max_denominator)\n",
    "    term = np.clip(term, 0, None)  # Ensure term is not negative\n",
    "    return np.sqrt(F_PQ_PC * term)\n",
    "\n",
    "# Parameters for the plots\n",
    "RR_opt = 0.9  # Fixed RR_opt value\n",
    "RR_values = np.linspace(0, 1, 500)  # Range of RR values\n",
    "F_PQ_PC_values = [0.1, 0.5, 0.98]  # Fixed F_PQ_PC values\n",
    "\n",
    "# Color-blind-friendly colors\n",
    "colors = ['#4daf4a', '#377eb8', '#ff7f00', '#f781bf']\n",
    "\n",
    "# Plot 1: PBS vs. RR for different F_PQ_PC values\n",
    "plt.figure(figsize=(12, 10))\n",
    "for idx, F_PQ_PC in enumerate(F_PQ_PC_values):\n",
    "    PBS_values = PBS(F_PQ_PC, RR_values, RR_opt)\n",
    "    plt.plot(RR_values, PBS_values, label=f'$F_{{PQ,PC}} = {F_PQ_PC}$', color=colors[idx], linewidth=6)\n",
    "\n",
    "plt.xlabel('$RR$', fontsize=50)\n",
    "plt.ylabel('$PBS$', fontsize=50)\n",
    "plt.axvline(RR_opt, color='black', linestyle='--', label=f'$RR_{{opt}} = {RR_opt}$', linewidth=6)\n",
    "plt.xticks(fontsize=45)\n",
    "plt.yticks(fontsize=45)\n",
    "plt.legend(fontsize=40, loc='upper left', bbox_to_anchor=(0.01, 0.99), borderaxespad=0, handletextpad=0.2, borderpad=0.2, labelspacing=0.2)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().set_xticks([0] + list(plt.gca().get_xticks())[1:])\n",
    "plt.gca().set_xticklabels([''] + [f'{x:.1f}' for x in plt.gca().get_xticks()[1:]])\n",
    "plt.gca().set_yticks([0] + list(plt.gca().get_yticks())[1:])\n",
    "plt.gca().set_yticklabels([''] + [f'{y:.1f}' for y in plt.gca().get_yticks()[1:]])\n",
    "plt.gca().annotate('0.0', xy=(0, 0), xytext=(-0.1, -0.05), textcoords='axes fraction', fontsize=45, ha='left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGS/PBS-RR.pdf')\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: PBS vs. F_PQ_PC for different RR values\n",
    "plt.figure(figsize=(12, 10))\n",
    "RR_test_values = [0.6, 0.8, RR_opt, 0.95]\n",
    "\n",
    "for idx, RR in enumerate(RR_test_values):\n",
    "    PBS_values = PBS(F_PQ_PC_range, RR, RR_opt)\n",
    "    linestyle = '--' if RR == RR_opt else '-'\n",
    "    plt.plot(F_PQ_PC_range, PBS_values, label=f'$RR = {RR}$' if RR != RR_opt else f'$RR_{{opt}} = {RR_opt}$', color=colors[idx], linewidth=6, linestyle=linestyle)\n",
    "\n",
    "plt.xlabel('$F_{PQ,PC}$', fontsize=50)\n",
    "plt.ylabel('$PBS$', fontsize=50)\n",
    "plt.xticks(fontsize=45)\n",
    "plt.yticks(fontsize=45)\n",
    "plt.legend(fontsize=40, loc='lower right', bbox_to_anchor=(0.99, 0.01), borderaxespad=0, handletextpad=0.2, borderpad=0.2, labelspacing=0.2)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().set_xticks([0] + list(plt.gca().get_xticks())[1:])\n",
    "plt.gca().set_xticklabels([''] + [f'{x:.1f}' for x in plt.gca().get_xticks()[1:]])\n",
    "plt.gca().set_yticks([0] + list(plt.gca().get_yticks())[1:])\n",
    "plt.gca().set_yticklabels([''] + [f'{y:.1f}' for y in plt.gca().get_yticks()[1:]])\n",
    "plt.gca().annotate('0.0', xy=(0, 0), xytext=(-0.1, -0.05), textcoords='axes fraction', fontsize=45, ha='left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGS/PBS-F.pdf')\n",
    "\n",
    "# Show the plots\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from func import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    QGramsBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    SuffixArraysBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    ")\n",
    "\n",
    "from DeepBlocksrc.deep_blocker import canopy_deep_blocker\n",
    "\n",
    "from pyjedai.comparison_cleaning import CardinalityEdgePruning\n",
    "from pyjedai.comparison_cleaning import BLAST\n",
    "import os\n",
    "\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "from pyjedai.comparison_cleaning import BLAST, CardinalityEdgePruning\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "classic_method_name = {\n",
    "    'CTT': 'CTT',\n",
    "    'AE': 'AUTO',\n",
    "}\n",
    "\n",
    "output_file = \"deepBlcok.txt\"\n",
    "\n",
    "# Remove output file if it exists\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Beer', 'Walmart-Amazon', 'Amazon-Google', 'DBLP-ACM', 'Fodors-Zagat', 'iTunes-Amazon', 'DBLP-GoogleScholar', 'Febrl']\n",
    "methods = ['AE', 'CTT']\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        \n",
    "        # Load datasets\n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "        # Clean datasets for specific tasks\n",
    "        if task == 'Fodors-Zagat':\n",
    "            for df in [left_df, right_df]:\n",
    "                df.applymap(lambda x: x.strip('`').strip() if isinstance(x, str) else x)\n",
    "                df.applymap(lambda x: x.strip(\"'\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "        left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "\n",
    "        # Process using classic method if applicable\n",
    "        # if method in classic_method_dict:\n",
    "            # bb = classic_method_dict[method]\n",
    "            # attr = [col for col in left_df.columns if col != 'id']\n",
    "            # data = Data(\n",
    "            #     dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "            #     dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "            #     ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'})\n",
    "            # )\n",
    "            # data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "            \n",
    "        start_time = time.time()\n",
    "        candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = method)\n",
    "\n",
    "        end_time = time.time()\n",
    "        candidates = candidates.astype(int)\n",
    "\n",
    "    # Compile result rows\n",
    "        left_merged = candidates.merge(left_df, left_on='id1', right_on='id', suffixes=('', '_left'))\n",
    "        right_merged = left_merged.merge(right_df, left_on='id2', right_on='id', suffixes=('_left', '_right'))\n",
    "\n",
    "        # Drop redundant columns and reset index\n",
    "        result_df = right_merged.drop(columns=['id1', 'id2', 'id_left', 'id_right'])\n",
    "        result_df = right_merged.copy()\n",
    "\n",
    "        # Merge with match_df to determine labels\n",
    "        merged_df = result_df.merge(match_df, left_on=['id_left', 'id_right'], right_on=['ltable_id', 'rtable_id'], how='left', indicator=True)\n",
    "        result_df['label'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        RR = 1 - result_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        RR_opt = 1 - match_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        PC = np.sum(result_df['label'] == 1) / match_df.shape[0]\n",
    "        PQ = np.sum(result_df['label'] == 1) / result_df.shape[0]\n",
    "        F = 2 * PC * PQ / (PC + PQ)\n",
    "\n",
    "        # Print and save results\n",
    "        print(task, classic_method_name[method])\n",
    "        print(round(100 * RR, 4), round(100 * PC, 4), round(100 * PQ, 4), round(100 * F, 4))\n",
    "        print()\n",
    "\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "            file.write(f\"{round(100 * RR, 4)} {round(100 * PC, 4)} {round(100 * PQ, 4)} {round(100 * F, 4)} {round(runtime, 2)}\\n\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(result_df['label'] == 1) / match_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task  ='DBLP-ACM'\n",
    "# task  ='Fodors-Zagat'\n",
    "\n",
    "# Load datasets\n",
    "left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "\n",
    "# import pickle\n",
    "# with open('blocking_result.pkl', 'rb') as file:\n",
    "#     pairs = pickle.load(file)\n",
    "\n",
    "\n",
    "# tmp = np.array(pairs)\n",
    "# tmp =tmp[:,0:2].astype(int)\n",
    "\n",
    "# candidates = pd.DataFrame({'id1':tmp[:,0],'id2':tmp[:,1]})\n",
    "\n",
    "\n",
    "candidates = pd.read_csv(\"tmp.csv\")\n",
    "# candidates.to_csv(hp['task']+'_pairs.csv', index=False)\n",
    "\n",
    "candidates= candidates.rename(columns={'id1':'1','id2':'2'})\n",
    "candidates = candidates.rename(columns={'2':'id1','1':'id2'})\n",
    "# candidates\n",
    "        # Rename columns in candidates DataFrame\n",
    "# Merge left_df and right_df with candidates based on ids using vectorized operations\n",
    "left_merged = candidates.merge(left_df, left_on='id1', right_on='id', suffixes=('', '_left'))\n",
    "right_merged = left_merged.merge(right_df, left_on='id2', right_on='id', suffixes=('_left', '_right'))\n",
    "\n",
    "# Drop redundant columns and reset index\n",
    "result_df = right_merged.drop(columns=['id1', 'id2', 'id_left', 'id_right'])\n",
    "result_df = right_merged.copy()\n",
    "\n",
    "# Merge with match_df to determine labels\n",
    "merged_df = result_df.merge(match_df, left_on=['id_left', 'id_right'], right_on=['ltable_id', 'rtable_id'], how='left', indicator=True)\n",
    "result_df['label'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "\n",
    "\n",
    "RR = 1 - result_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "RR_opt = 1 - match_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "PC = np.sum(result_df['label'] == 1) / match_df.shape[0]\n",
    "PQ = np.sum(result_df['label'] == 1) / result_df.shape[0]\n",
    "F = 2 * PC * PQ / (PC + PQ)\n",
    "\n",
    "# Print and save results\n",
    "print(task)\n",
    "print(round(100 * RR, 4), round(100 * PC, 4), round(100 * PQ, 4))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from func import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "classic_method_name = {\n",
    "    'CTT': 'CTT',\n",
    "    'AE': 'AUTO',\n",
    "}\n",
    "\n",
    "output_file = \"deepBlcok.txt\"\n",
    "\n",
    "# Remove output file if it exists\n",
    "try: os.remove(output_file)\n",
    "except: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Beer', 'Walmart-Amazon', 'Amazon-Google', 'DBLP-ACM', 'Fodors-Zagat', 'iTunes-Amazon', 'DBLP-GoogleScholar', 'Febrl']\n",
    "methods = ['AE', 'CTT']\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        \n",
    "        # Load datasets\n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "        # Clean datasets for specific tasks\n",
    "        if task == 'Fodors-Zagat':\n",
    "            for df in [left_df, right_df]:\n",
    "                df.applymap(lambda x: x.strip('`').strip() if isinstance(x, str) else x)\n",
    "                df.applymap(lambda x: x.strip(\"'\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "        left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = method)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Rename columns in candidates DataFrame\n",
    "        candidates.rename(columns={'ltable_id': 'id1', 'rtable_id': 'id2'}, inplace=True)\n",
    "\n",
    "        # Merge left_df and right_df with candidates based on ids using vectorized operations\n",
    "        left_merged = candidates.merge(left_df, left_on='id1', right_on='id', suffixes=('', '_left'))\n",
    "        right_merged = left_merged.merge(right_df, left_on='id2', right_on='id', suffixes=('_left', '_right'))\n",
    "\n",
    "        # Drop redundant columns and reset index\n",
    "        result_df = right_merged.drop(columns=['id1', 'id2', 'id_left', 'id_right'])\n",
    "        result_df = right_merged.copy()\n",
    "\n",
    "        # Merge with match_df to determine labels\n",
    "        merged_df = result_df.merge(match_df, left_on=['id_left', 'id_right'], right_on=['ltable_id', 'rtable_id'], how='left', indicator=True)\n",
    "        result_df['label'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        RR = 1 - result_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        RR_opt = 1 - match_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        PC = np.sum(result_df['label'] == 1) / match_df.shape[0]\n",
    "        PQ = np.sum(result_df['label'] == 1) / result_df.shape[0]\n",
    "        F = 2 * PC * PQ / (PC + PQ)\n",
    "\n",
    "        # Print and save results\n",
    "        print(task, classic_method_name[method])\n",
    "        print(round(100 * RR, 4), round(100 * PC, 4), round(100 * PQ, 4), round(100 * F, 4))\n",
    "        print()\n",
    "\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "            file.write(f\"{round(100 * RR, 4)} {round(100 * PC, 4)} {round(100 * PQ, 4)} {round(100 * F, 4)} {round(runtime, 2)}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparkly.index import IndexConfig, LuceneIndex\n",
    "from sparkly.search import Searcher\n",
    "from pathlib import Path\n",
    "\n",
    "# the number of candidates returned per record\n",
    "limit = 50\n",
    "# path to the test data\n",
    "data_path = Path('./examples/data/abt_buy/').absolute()\n",
    "# table to be indexed\n",
    "table_a_path = data_path / 'table_a.parquet'\n",
    "# table for searching\n",
    "table_b_path = data_path / 'table_b.parquet'\n",
    "# the ground truth\n",
    "gold_path = data_path / 'gold.parquet'\n",
    "# the analyzers used to convert the text into tokens for indexing\n",
    "analyzers = ['3gram']\n",
    "\n",
    "# initialize a local spark context\n",
    "spark = SparkSession.builder\\\n",
    "                    .master('local[*]')\\\n",
    "                    .appName('Sparkly Example')\\\n",
    "                    .getOrCreate()\n",
    "# read all the data as spark dataframes\n",
    "table_a = spark.read.parquet(f'file://{str(table_a_path)}')\n",
    "table_b = spark.read.parquet(f'file://{str(table_b_path)}')\n",
    "gold = spark.read.parquet(f'file://{str(gold_path)}')\n",
    "# the index config, '_id' column will be used as the unique \n",
    "# id column in the index. Note id_col must be an integer (32 or 64 bit)\n",
    "config = IndexConfig(id_col='_id')\n",
    "# add the 'name' column to be indexed with analyzer above\n",
    "config.add_field('name', analyzers)\n",
    "# create a new index stored at /tmp/example_index/\n",
    "index = LuceneIndex('/tmp/example_index/', config)\n",
    "# index the records from table A according to the config we created above\n",
    "index.upsert_docs(table_a)\n",
    "\n",
    "# get a query spec (template) which searches on \n",
    "# all indexed fields\n",
    "query_spec = index.get_full_query_spec()\n",
    "# create a searcher for doing bulk search using our index\n",
    "searcher = Searcher(index)\n",
    "# search the index with table b\n",
    "candidates = searcher.search(table_b, query_spec, id_col='_id', limit=limit).cache()\n",
    "\n",
    "candidates.show()\n",
    "# output is rolled up \n",
    "# search record id -> (indexed ids + scores + search time)\n",
    "#\n",
    "# explode the results to compute recall\n",
    "pairs = candidates.select(\n",
    "                    F.explode('ids').alias('a_id'),\n",
    "                    F.col('_id').alias('b_id')\n",
    "                )\n",
    "# number of matches found\n",
    "true_positives = gold.intersect(pairs).count()\n",
    "# precentage of matches found\n",
    "recall = true_positives / gold.count()\n",
    "\n",
    "print(f'true_positives : {true_positives}')\n",
    "print(f'recall : {recall}')\n",
    "\n",
    "candidates.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR = []\n",
    "PC = []\n",
    "PQ = []\n",
    "correlation_combined_pq_all =[]\n",
    "correlation_combined_pc_all = []\n",
    "correlation_combined_rr_all = []\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "# for i in [1]:\n",
    "\n",
    "    RR = []\n",
    "    PC = []\n",
    "    PQ = []\n",
    "    # for method in list(METHODS.keys()):\n",
    "    for method in list(METHODS.keys()):\n",
    "        for task in [TASKS[i]]:\n",
    "             \n",
    "        \n",
    "            row = res[task][method]\n",
    "            RR.append(row['RR'])\n",
    "            PC.append(row['PC'])\n",
    "            PQ.append(row['PQ'])\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Example data (replace these with your actual data arrays)\n",
    "\n",
    "    # Create a DataFrame for easier handling\n",
    "    data = pd.DataFrame({'RR': RR, 'PC': PC, 'PQ': PQ})\n",
    "    \n",
    "    \n",
    "    # data_ = pd.DataFrame({'RR': RR, 'PC': PC, 'PQ': PQ})\n",
    "    # scaler = StandardScaler()\n",
    "    # data = scaler.fit_transform(data_)\n",
    "    # data = pd.DataFrame({'RR': data[:,0], 'PC': data[:,1], 'PQ': data[:,2]})\n",
    "\n",
    "\n",
    "\n",
    "    # 1. Correlation of PQ with combined RR and PC\n",
    "    X1 = data[['RR', 'PC']]  # Independent variables (RR, PC)\n",
    "    y1 = data['PQ']           # Dependent variable (PQ)\n",
    "\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X1, y1)\n",
    "    pq_pred = model1.predict(X1)\n",
    "    correlation_combined_pq = np.corrcoef(y1, pq_pred)[0, 1]\n",
    "    print(f\"Correlation of PQ with combined RR and PC: {correlation_combined_pq:.2f}\")\n",
    "\n",
    "    # 2. Correlation of RR with combined PC and PQ\n",
    "    X2 = data[['PC', 'PQ']]  # Independent variables (PC, PQ)\n",
    "    y2 = data['RR']           # Dependent variable (RR)\n",
    "\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X2, y2)\n",
    "    rr_pred = model2.predict(X2)\n",
    "    correlation_combined_rr = np.corrcoef(y2, rr_pred)[0, 1]\n",
    "    print(f\"Correlation of RR with combined PC and PQ: {correlation_combined_rr:.2f}\")\n",
    "\n",
    "    # 3. Correlation of PC with combined RR and PQ\n",
    "    X3 = data[['RR', 'PQ']]  # Independent variables (RR, PQ)\n",
    "    y3 = data['PC']           # Dependent variable (PC)\n",
    "\n",
    "    model3 = LinearRegression()\n",
    "    model3.fit(X3, y3)\n",
    "    pc_pred = model3.predict(X3)\n",
    "    correlation_combined_pc = np.corrcoef(y3, pc_pred)[0, 1]\n",
    "    print(f\"Correlation of PC with combined RR and PQ: {correlation_combined_pc:.4f}\")\n",
    "    print()\n",
    "\n",
    "    correlation_combined_pq_all.append(correlation_combined_pq)\n",
    "    correlation_combined_rr_all.append(correlation_combined_rr)\n",
    "    correlation_combined_pc_all.append(correlation_combined_pc)\n",
    "\n",
    "np.mean(correlation_combined_pq_all) , np.mean(correlation_combined_rr_all) , np.mean(correlation_combined_pc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from func import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    "    QGramsBlocking,\n",
    "    SuffixArraysBlocking\n",
    ")\n",
    "from pyjedai.comparison_cleaning import BLAST, CardinalityEdgePruning\n",
    "\n",
    "\n",
    "# Define method dictionaries\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "classic_method_name = {\n",
    "    'SB': 'StandardBlocking',\n",
    "    'EQG': 'ExtendedQGramsBlocking',\n",
    "    'ESA': 'ExtendedSuffixArraysBlocking',\n",
    "    'QG': 'QGramsBlocking',\n",
    "    'SA': 'SuffixArraysBlocking',\n",
    "    'CTT': 'CTT',\n",
    "    'AE': 'AUTO',\n",
    "\n",
    "}\n",
    "\n",
    "import random \n",
    "\n",
    "def shuffle_string(s):\n",
    "    str_list = list(s)\n",
    "    random.shuffle(str_list)\n",
    "    shuffled_str = ''.join(str_list)\n",
    "    return shuffled_str\n",
    "\n",
    "def shuffle_df(df_in,match_list_in, frac  =10):\n",
    "    df = df_in.copy()\n",
    "    match_list = match_list_in.copy()\n",
    "    np.random.shuffle(match_list)\n",
    "    R = int(len(match_list) * (frac /100))\n",
    "    for i in match_list[0:R]: \n",
    "        for col in list(df.columns):\n",
    "            if col =='id': continue\n",
    "            \n",
    "            s = str(df[df['id'] == i][col].to_list()[0])\n",
    "            s2 = shuffle_string(s)\n",
    "            df.loc[df['id'] == i, col] = s2\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "output_file = \"tmp2.txt\"\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "Time_all = []\n",
    "\n",
    "tasks = ['Beer', 'Walmart-Amazon', 'Amazon-Google', 'DBLP-ACM', 'Fodors-Zagat', 'iTunes-Amazon', 'DBLP-GoogleScholar', 'Febrl']\n",
    "\n",
    "\n",
    "task = 'Amazon-Google'\n",
    "methods = ['SB', 'EQG', 'ESA', 'QG','SA','CTT','AE']\n",
    "methods = ['SB']\n",
    "EXP2 = []\n",
    "\n",
    "# Process each task with each method\n",
    "for method in methods:\n",
    "    \n",
    "    # Load datasets\n",
    "    left_df_in = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "    right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "    match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "\n",
    "    for F in [0,10,20,30,40,50,60,70,80,90,100]:\n",
    "    # for F in [0,25,50,75,100]:\n",
    "    # for F in [0,50,100]:\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        for _ in range(4):\n",
    "            match_left = np.unique(list(match_df['ltable_id']))\n",
    "            np.random.shuffle(match_left)\n",
    "            left_df = shuffle_df(left_df_in.copy(),match_left, frac  =F)\n",
    "\n",
    "            left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "            right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "\n",
    "            # Process using classic method if applicable\n",
    "\n",
    "            \n",
    "\n",
    "            if method == 'CTT':\n",
    "                start_time = time.time()\n",
    "                candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = 'CTT')\n",
    "            elif method =='AE':\n",
    "                start_time = time.time()\n",
    "                candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = 'AE')\n",
    "            else:\n",
    "\n",
    "                bb = classic_method_dict[method]\n",
    "                attr = [col for col in left_df.columns if col != 'id']\n",
    "                data = Data(\n",
    "                    dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "                    dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "                    ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'})\n",
    "                )\n",
    "                data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "                start_time = time.time()\n",
    "                blocks = bb.build_blocks(copy.deepcopy(data), attributes_1=attr, attributes_2=attr, tqdm_disable=True)\n",
    "                bp = BlockPurging()\n",
    "                bf = BlockFiltering()\n",
    "                mb = BLAST('EJS')\n",
    "                cleaned_blocks = bf.process(blocks, data, tqdm_disable=True)\n",
    "                filtered_blocks = bp.process(cleaned_blocks, data, tqdm_disable=True)\n",
    "                # candidate_pairs_blocks = mb.process(filtered_blocks, data, tqdm_disable=False)\n",
    "\n",
    "                # candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "            end_time = time.time()\n",
    "\n",
    "            runtime = end_time - start_time\n",
    "            Time_all.append(runtime)\n",
    "\n",
    "\n",
    "\n",
    "        # Print and save results\n",
    "\n",
    "        runtime = np.mean(Time_all)\n",
    "        runtime_std = np.std(Time_all)\n",
    "        EXP2.append([F, runtime, runtime_std])\n",
    "        print(task, classic_method_name[method],round(runtime, 3), round(runtime_std,3))\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_values = np.array(EXP2)[:,0]\n",
    "averages = np.array(EXP2)[:,1]\n",
    "std_devs = np.array(EXP2)[:,2]\n",
    "color = '#0072B2'  # A blue color that is colorblind-friendly\n",
    "\n",
    "# Plotting the data with error bars\n",
    "plt.errorbar(x_values, averages, yerr=std_devs, fmt='o', capsize=5, color=color, label='Data with error bars')\n",
    "\n",
    "# Adding line to connect the dots with the same color\n",
    "plt.plot(x_values, averages, linestyle='-', marker='o', color=color, label='Connected Line')\n",
    "\n",
    "plt.xlabel('fraction of disturbance in true matches')\n",
    "plt.ylabel('time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from func import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyjedai.datamodel import Data\n",
    "from pyjedai.block_cleaning import BlockPurging, BlockFiltering\n",
    "from pyjedai.block_building import (\n",
    "    StandardBlocking,\n",
    "    ExtendedQGramsBlocking,\n",
    "    ExtendedSuffixArraysBlocking,\n",
    "    QGramsBlocking,\n",
    "    SuffixArraysBlocking\n",
    ")\n",
    "from pyjedai.comparison_cleaning import (\n",
    "    WeightedEdgePruning, WeightedNodePruning, \n",
    "    CardinalityEdgePruning, CardinalityNodePruning,\n",
    "    BLAST, ReciprocalCardinalityNodePruning,\n",
    "    ReciprocalWeightedNodePruning, ComparisonPropagation)\n",
    "\n",
    "\n",
    "# Define method dictionaries\n",
    "classic_method_dict = {\n",
    "                'SB': StandardBlocking(),\n",
    "                'QG': QGramsBlocking(),\n",
    "                'EQG': ExtendedQGramsBlocking(),\n",
    "                'SA': SuffixArraysBlocking(),\n",
    "                'ESA': ExtendedSuffixArraysBlocking()\n",
    "                }\n",
    "classic_method_name = {\n",
    "    'SB': 'StandardBlocking',\n",
    "    'EQG': 'ExtendedQGramsBlocking',\n",
    "    'ESA': 'ExtendedSuffixArraysBlocking',\n",
    "    'QG': 'QGramsBlocking',\n",
    "    'SA': 'SuffixArraysBlocking',\n",
    "    'CTT': 'CTT',\n",
    "    'AE': 'AUTO',\n",
    "\n",
    "}\n",
    "\n",
    "output_file = \"block_stat.txt\"\n",
    "# output_file = \"tmp.txt\"\n",
    "\n",
    "# Remove output file if it exists\n",
    "try: os.remove(output_file)\n",
    "except FileNotFoundError: pass\n",
    "\n",
    "# Define tasks and methods to iterate over\n",
    "tasks = ['Beer', 'Walmart-Amazon', 'Amazon-Google', 'DBLP-ACM', 'Fodors-Zagat', 'DBLP-GoogleScholar']# 'iTunes-Amazon'\n",
    "tasks = ['Beer']\n",
    "methods = ['SB', 'EQG', 'ESA', 'QG','SA','CTT','AE']\n",
    "\n",
    "\n",
    "\n",
    "# Process each task with each method\n",
    "for task in tasks:\n",
    "    for method in methods:\n",
    "        \n",
    "        # Load datasets\n",
    "        left_df = pd.read_csv(f\"data/{task}/tableA.csv\")\n",
    "        right_df = pd.read_csv(f\"data/{task}/tableB.csv\")\n",
    "        match_df = pd.read_csv(f\"data/{task}/matches.csv\")\n",
    "\n",
    "        # Clean datasets for specific tasks\n",
    "        if task == 'Fodors-Zagat':\n",
    "            for df in [left_df, right_df]:\n",
    "                df.applymap(lambda x: x.strip('`').strip() if isinstance(x, str) else x)\n",
    "                df.applymap(lambda x: x.strip(\"'\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "        left_df = left_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        right_df = right_df.replace(r\"\\\\ '\", \"'\", regex=True).replace(r\" '\", \"'\", regex=True).replace(r\"\\\\ `\", \"\\\\ \", regex=True)\n",
    "        \n",
    "        left_df = left_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        right_df = right_df.applymap(lambda x: x.strip('`') if isinstance(x, str) else x).applymap(lambda x: x.strip(\"'\") if isinstance(x, str) else x).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "\n",
    "        # Process using classic method if applicable\n",
    "        if method in classic_method_dict:\n",
    "            bb = classic_method_dict[method]\n",
    "            attr = [col for col in left_df.columns if col != 'id']\n",
    "            data = Data(\n",
    "                dataset_1=left_df.copy(), id_column_name_1='id',\n",
    "                dataset_2=right_df.copy(), id_column_name_2='id',\n",
    "                ground_truth=match_df.rename(columns={list(match_df.columns)[0]: 'D1', list(match_df.columns)[1]: 'D2'})\n",
    "            )\n",
    "            data.clean_dataset(remove_stopwords=False, remove_punctuation=False, remove_numbers=False, remove_unicodes=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            blocks = bb.build_blocks(copy.deepcopy(data), attributes_1=attr, attributes_2=attr, tqdm_disable=True)\n",
    "            \n",
    "            bp = BlockPurging()\n",
    "            bf = BlockFiltering()\n",
    "            mb = BLAST('EJS')\n",
    "            # if task == 'iTunes-Amazon':\n",
    "            #     mb = CardinalityEdgePruning()\n",
    "            # for meta in META:\n",
    "            cleaned_blocks = bf.process(copy.deepcopy(blocks), data, tqdm_disable=True)\n",
    "            filtered_blocks = bp.process(cleaned_blocks, data, tqdm_disable=True)\n",
    "            candidate_pairs_blocks = mb.process(filtered_blocks, data, tqdm_disable=False)\n",
    "\n",
    "            candidates = mb.export_to_df(candidate_pairs_blocks)\n",
    "        \n",
    "        else:\n",
    "            if method == 'CTT':\n",
    "                start_time = time.time()\n",
    "                candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = 'CTT')\n",
    "            elif method =='AE':\n",
    "                start_time = time.time()\n",
    "                candidates = deepBlock(left_df.copy(), right_df.copy(), K = 50, method = 'AE')\n",
    "            else:\n",
    "                print('method not found!')\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Compile result rows\n",
    "        candidates= candidates.astype(int)\n",
    "\n",
    "        # Rename columns in candidates DataFrame\n",
    "        candidates.rename(columns={'ltable_id': 'id1', 'rtable_id': 'id2'}, inplace=True)\n",
    "\n",
    "        # Merge left_df and right_df with candidates based on ids using vectorized operations\n",
    "        left_merged = candidates.merge(left_df, left_on='id1', right_on='id', suffixes=('', '_left'))\n",
    "        right_merged = left_merged.merge(right_df, left_on='id2', right_on='id', suffixes=('_left', '_right'))\n",
    "\n",
    "        # Drop redundant columns and reset index\n",
    "        result_df = right_merged.drop(columns=['id1', 'id2', 'id_left', 'id_right'])\n",
    "        result_df = right_merged.copy()\n",
    "\n",
    "        # Merge with match_df to determine labels\n",
    "        merged_df = result_df.merge(match_df, left_on=['id_left', 'id_right'], right_on=['ltable_id', 'rtable_id'], how='left', indicator=True)\n",
    "        result_df['label'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "        # result_df.to_csv(task +'_'+method+ '.csv',index= False)\n",
    "\n",
    "        # Calculate metrics\n",
    "        RR = 1 - result_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        RR_opt = 1 - match_df.shape[0] / (left_df.shape[0] * right_df.shape[0])\n",
    "        PC = np.sum(result_df['label'] == 1) / match_df.shape[0]\n",
    "        PQ = np.sum(result_df['label'] == 1) / result_df.shape[0]\n",
    "        F = 2 * PC * PQ / (PC + PQ)\n",
    "\n",
    "        # Print and save results\n",
    "        print(task, classic_method_name[method])\n",
    "        print(round(100 * RR, 4), round(100 * PC, 4), round(100 * PQ, 4), round(100 * F, 4))\n",
    "        print()\n",
    "\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "        MINOR, MAJOR, data_STAT = calc_bias_block(result_df,match_df,left_df,right_df, task , sens_dict)\n",
    "        [RR_minor,PC_minor,PQ_minor,Fb_minor ] = MINOR\n",
    "        [RR_major,PC_major,PQ_major,Fb_major ] = MAJOR\n",
    "        [P_major, P_minor, M_major, M_minor] = data_STAT\n",
    "\n",
    "        NUM = 2\n",
    "        print('Minor: ',end='')\n",
    "        print(round(RR_minor,NUM),round(PC_minor,NUM),round(PQ_minor,NUM), round(Fb_minor,NUM))\n",
    "        print('major: ',end='')\n",
    "        print(round(RR_major,NUM),round(PC_major,NUM),round(PQ_major,NUM), round(Fb_major,NUM))\n",
    "        print('diff : ',end='')\n",
    "        print(round(RR_major-RR_minor ,NUM),round(PC_major- PC_minor,NUM),round(PQ_major - PQ_minor,NUM),round(Fb_major - Fb_minor,NUM))\n",
    "\n",
    "\n",
    "        print()\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{task} {classic_method_name[method]}\\n\")\n",
    "            file.write(f\"{round(100 * RR, 4)} {round(100 * PC, 4)} {round(100 * PQ, 4)} {round(100 * F, 4)} {round(runtime, 2)}\\n\\n\")\n",
    "            file.write(f\"bias {round( RR_minor, 4)} {round( PC_minor, 4)} {round( PQ_minor, 4)} {round( Fb_minor, 4)}\\n\\n\")\n",
    "            file.write(f\"bias {round( RR_major, 4)} {round( PC_major, 4)} {round( PQ_major, 4)} {round( Fb_major, 4)}\\n\\n\")\n",
    "            file.write(f\"bias {round( RR_major - RR_minor, 4)} {round( PC_major - PC_minor, 4)} {round( PQ_major - PQ_minor, 4)} {round( Fb_major - Fb_minor, 4)}\\n\\n\")\n",
    "            file.write(f\"bias {P_major} {P_minor} {M_major} {M_minor}\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your text file\n",
    "file_path = 'block_stat.txt'\n",
    "\n",
    "# Read the file and process the data\n",
    "data = []\n",
    "res = {}\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into parts based on spaces\n",
    "        parts = line.split()\n",
    "        if parts == []: continue\n",
    "        if len(parts) == 2:\n",
    "            if parts[0] not in res.keys():\n",
    "                res[parts[0]] = {}\n",
    "            res[parts[0]][parts[1]] = {}\n",
    "            method = parts[1]\n",
    "            dataset = parts[0]\n",
    "            cnt = 0\n",
    "        elif parts[0] != 'bias':\n",
    "            RR = float(parts[0])\n",
    "            PC = float(parts[1])\n",
    "            PQ = float(parts[2])\n",
    "            time_ = float(parts[4])\n",
    "            Fb = 2*PC *RR / (PC + RR)\n",
    "            res[dataset][method] = {'RR':round(RR,5), 'PC':round(PC,5), 'PQ':round(PQ,5), 'Fb':round(Fb,5), 'time':round(time_,5)}\n",
    "        else:\n",
    "            if cnt ==0:\n",
    "                RR_minor = float(parts[1])\n",
    "                PC_minor = float(parts[2])\n",
    "                PQ_minor = float(parts[3])\n",
    "                Fb_minor = float(parts[4])\n",
    "                res[dataset][method]['RR_minor'] = round(RR_minor,5)\n",
    "                res[dataset][method]['PC_minor'] = round(PC_minor,5)\n",
    "                res[dataset][method]['PQ_minor'] = round(PQ_minor,5)\n",
    "                res[dataset][method]['Fb_minor'] = round(Fb_minor,5)\n",
    "                cnt+=1\n",
    "            elif cnt ==1:\n",
    "                RR_major = float(parts[1])\n",
    "                Pc_major = float(parts[2])\n",
    "                PQ_major = float(parts[3])\n",
    "                Fb_major = float(parts[4])\n",
    "                res[dataset][method]['RR_major'] = round(RR_major,5)\n",
    "                res[dataset][method]['Pc_major'] = round(Pc_major,5)\n",
    "                res[dataset][method]['PQ_major'] = round(PQ_major,5)\n",
    "                res[dataset][method]['Fb_major'] = round(Fb_major,5)\n",
    "                cnt+=1\n",
    "            elif cnt==2:\n",
    "\n",
    "                RR_diff = float(parts[1])\n",
    "                Pc_diff = float(parts[2])\n",
    "                PQ_diff = float(parts[3])\n",
    "                Fb_diff = float(parts[4])\n",
    "                res[dataset][method]['RR_diff'] = round(RR_diff,5)\n",
    "                res[dataset][method]['Pc_diff'] = round(Pc_diff,5)\n",
    "                res[dataset][method]['PQ_diff'] = round(PQ_diff,5)\n",
    "                res[dataset][method]['Fb_diff'] = round(Fb_diff,5)\n",
    "                cnt+=1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "METHODS = {\n",
    "    'StandardBlocking':'\\\\stdBlock',\n",
    "    'QGramsBlocking':'\\\\qgram',\n",
    "    'ExtendedQGramsBlocking':'\\\\exQgram',\n",
    "    'SuffixArraysBlocking':'\\\\suffix',\n",
    "    'ExtendedSuffixArraysBlocking':'\\\\exSuffix',\n",
    "    'AUTO':'\\\\AutoBlock',\n",
    "    'CTT':'\\\\CTT'}\n",
    "\n",
    "\n",
    "\n",
    "TASKS = ['Amazon-Google', 'Walmart-Amazon', 'DBLP-GoogleScholar', 'DBLP-ACM', 'Beer', 'Fodors-Zagat', 'iTunes-Amazon']\n",
    "TASKS = ['DBLP-GoogleScholar', 'DBLP-ACM','Beer']\n",
    "# TASKS = ['Amazon-Google', 'Walmart-Amazon','Beer', 'Fodors-Zagat']\n",
    "\n",
    "\n",
    "# Open a file in write mode\n",
    "with open('exp1__latex.txt', 'w') as file:\n",
    "    for method in METHODS.keys():\n",
    "        for task in TASKS:\n",
    "            row = res[task][method]\n",
    "\n",
    "            Fb = 2 *row['PC'] * row['RR'] /(row['PC'] + row['RR'])\n",
    "            if task == 'iTunes-Amazon':\n",
    "                file.write(f\"{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {Fb:.2f} \\\\\\\\\\n\")\n",
    "            elif task == 'Amazon-Google':\n",
    "                file.write(f\"{METHODS[method]} & \\n{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {Fb:.2f} & \\n\")\n",
    "            else:\n",
    "                file.write(f\"{row['RR']:.2f} & {row['PC']:.2f} & {row['PQ']:.2f} & {Fb:.2f} & \\n\")\n",
    "\n",
    "        file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Open a file in write mode\n",
    "with open('exp2__latex.txt', 'w') as file:\n",
    "    for method in METHODS.keys():\n",
    "        for task in TASKS:\n",
    "            \n",
    "            row = res[task][method]\n",
    "            Fb_minor = 2 *row['PC_minor'] * row['RR_minor'] /(row['PC_minor'] + row['RR_minor'])\n",
    "            Fb_major = 2 *row['Pc_major'] * row['RR_major'] /(row['Pc_major'] + row['RR_major'])\n",
    "\n",
    "            fb_diff = Fb_major - Fb_minor\n",
    "\n",
    "            if task == 'iTunes-Amazon':\n",
    "                file.write(f\"{row['RR_diff']:.2f} & {row['Pc_diff']:.2f} & {fb_diff:.2f}  \\\\\\\\\\n\")\n",
    "            elif task == 'Amazon-Google':\n",
    "                file.write(f\"{METHODS[method]} & \\n{row['RR_diff']:.2f} & {row['Pc_diff']:.2f} & {fb_diff:.2f} & \\n\")\n",
    "            else:\n",
    "                file.write(f\"{row['RR_diff']:.2f} & {row['Pc_diff']:.2f} & {fb_diff:.2f} & \\n\")\n",
    "\n",
    "        file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Open a file in write mode\n",
    "with open('exp3__latex.txt', 'w') as file:\n",
    "    for method in METHODS.keys():\n",
    "        for task in TASKS:\n",
    "            row = res[task][method]\n",
    "            Fb_minor = 2 *row['PC_minor'] * row['RR_minor'] /(row['PC_minor'] + row['RR_minor'])\n",
    "            Fb_major = 2 *row['Pc_major'] * row['RR_major'] /(row['Pc_major'] + row['RR_major'])\n",
    "\n",
    "\n",
    "\n",
    "            if task == 'Fodors-Zagat':\n",
    "                file.write(f\"{row['RR_minor']:.2f} & {row['RR_major']:.2f} & {row['PC_minor']:.2f} & {row['Pc_major']:.2f} & {Fb_minor:.2f} &  {Fb_major:.2f} \\\\\\\\\\n\")\n",
    "            elif task == 'Amazon-Google':\n",
    "                file.write(f\"{METHODS[method]} & \\n{row['RR_minor']:.2f} & {row['RR_major']:.2f} & {row['PC_minor']:.2f} & {row['Pc_major']:.2f} & {Fb_minor:.2f} &  {Fb_major:.2f} & \\n\")\n",
    "            elif task in ['Walmart-Amazon', 'Beer']:\n",
    "                file.write(f\"{row['RR_minor']:.2f} & {row['RR_major']:.2f} & {row['PC_minor']:.2f} & {row['Pc_major']:.2f} & {Fb_minor:.2f} &  {Fb_major:.2f} & \\n\")\n",
    "\n",
    "        file.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
